#　Classification and Regression Using Supervised Learning

在本章中，我们将使用监督学习技术来学习数据的分类和回归。在本章的最后，您将对这些主题有更好的理解：

- 监督学习与无监督学习之间的区别
- 分类方法
- 数据预处理方法
- 标签编码
- Logistic回归分类器
- 贝叶斯分类器
- 混淆矩阵
- 支持向量机和SVM分类器
- 线性和多项式回归 
- 单变量和多变量线性回归器
- 使用支持向量回归机估算房价

# 有监督与无监督学习
> 从流行媒体上不难看出，当今人工智能领域最热门的领域之一是机器学习。机器学习通常分为有监督学习和无监督学习。还存在其他分类，但我们将在后面讨论。

在给出更正式的定义之前，让我们对监督学习与非监督学习有一些直观的了解。假设您有一组人物肖像。这一组中的人是一个非常多样化的男人和女人，您具有各种国籍，年龄，体重等。最初，您通过无监督学习算法对数据集进行了处理。在这种情况下，在没有任何先验知识的情况下，无监督算法将根据其识别为相似的某些特征开始对这些照片进行分类。例如，它可能独自开始认识到男人和女人是不同的，并且可能开始将男人分组为一组，将女人分组为另一组。但是不能保证它将找到该模式。它可能会聚类图像，因为某些肖像具有深色背景，而其他肖像则具有浅色背景，这可能是无用的推断。

现在拍摄同一组照片，但是这次我们也为每张照片贴上标签。假设标签是性别。因为我们现在有了数据的标签，所以我们可以通过监督算法将此数据放入并使用输入变量（在这种情况下，输入变量是照片像素）来计算目标变量（在这种情况下，性别）。更正式地：

监督学习是指基于标记的训练数据构建机器学习模型的过程。在监督学习中，每个示例或行都是一个由输入变量和所需目标变量组成的元组。例如，机器学习中使用的常见数据集是“泰坦尼克号”数据集。该数据集包含描述著名船只 达铁尼号 的乘客的特征。一些输入功能是：

- Passenger name
- Sex 
- 客舱等级 Cabin class
- Age
- 登船地点 Place of embarkment

在这种情况下，目标变量将是乘客是否幸存。

无监督学习是指在不依赖标记的训练数据的情况下构建机器学习模型的过程。从某种意义上说，这与监督学习相反。由于没有可用的标签，因此您仅需要根据提供给您的数据来提取见解。通过无监督学习，我们正在训练一个系统，其中单独的数据点可能会分成多个群集或组。需要重点强调的一点是，我们并不确切地知道分离的标准是什么。因此，无监督学习算法需要以可能的最佳方式将给定的数据集分为几组。

既然我们已经描述了机器学习方法的主要分类方法之一，那么让我们开始研究如何对数据进行分类。

# What is classification?
> 在本节中，我们将讨论监督分类技术。分类过程是一种用于将数据排列到固定数量的类别中的技术，以便可以有效地使用它

在机器学习中，分类用于标识新数据点所属的类别。基于包含数据点和相应标签的训练数据集建立分类模型。例如，假设我们要确定一个给定的图像是否包含一个人的脸。我们将构建一个训练数据集，其中包含与两个类别相对应的类别：面部和无面部。然后将基于可用的训练样本来训练模型。然后可以将训练后的模型用于推理。

在机器学习中，分类用于标识新数据点所属的类别。基于包含数据点和相应标签的训练数据集建立分类模型。例如，假设我们要确定一个给定的图像是否包含一个人的脸。我们将构建一个训练数据集，其中包含与两个类别相对应的类别：面部和无面部。然后将基于可用的训练样本来训练模型。然后可以将训练后的模型用于推理。

良好的分类系统使查找和检索数据变得容易。分类被广泛用于面部识别，垃圾邮件识别，推荐引擎等。好的数据分类算法将自动生成正确的标准，以将给定的数据分为给定数量的类。

为了进行分类以产生体面的结果，将需要足够数量的样本，以便可以概括这些标准。如果样本数量不足，该算法将过度适合训练数据。这意味着它无法很好地处理未知数据，因为它对模型进行了微调，以致无法适应训练数据中观察到的模式。这实际上是机器学习领域中常见的问题。在构建各种机器学习模型时，考虑此因素是一个好主意。

# Preprocessing data

原始数据是机器学习算法的推动力。但是就像我们不能将原油放进汽车中，而是必须使用汽油一样，机器学习算法希望在训练过程开始之前就可以以某种方式格式化数据。为了准备通过机器学习算法提取的数据，必须对数据进行预处理并将其转换为正确的格式。让我们看一下实现这一目标的一些方法。

# Binarization
二值化用于将数值转换为布尔值。让我们使用一种内置方法，以2.1作为阈值对输入数据进行二值化处理。

# Mean removal
去除均值是机器学习中常用的预处理技术。从特征向量中删除平均值通常很有用，这样每个特征都以零为中心。我们这样做是为了消除特征向量中的特征偏差。

# Scaling
正如我们在前面各节中所做的那样，让我们通过访问一个示例来直观了解什么是scaling。

假设您有一个包含与房屋相关的特征的数据集，并且您正在尝试预测这些房屋的价格。这些功能的数值范围可能会大不相同。例如，房屋的平方英尺通常为数千，而房间数通常小于10。此外，其中一些功能可能包含一些异常值。例如，我们的数据集中可能会有一些大厦使其余的数据集倾斜。

我们需要找到一种缩放这些特征的方法，以使赋予每个特征的权重大致相同，而离群值的重要性也不会太大。

一种方法是重新调整所有功能，以使它们落在0和1之类的较小范围内。MinMaxScaler算法可能是实现此功能的最有效方法。

# 归一化 (Normalization)
>人们经常混淆缩放和归一化。

术语经常被混淆的原因之一是因为它们实际上非常相似。在这两种情况下，您都在转换数据以使数据更有用。但是，在缩放时，您正在更改变量的值范围，而在进行归一化时，您正在更改数据分布的形状。为了使机器学习模型更好地工作，希望特征的值呈正态分布。

但是现实是混乱的，有时情况并非如此。例如，值的分布可能会偏斜。规范化通常会分配数据。**

我们使用归一化过程来修改特征向量中的值，以便我们可以在一个通用尺度上对其进行测量。在机器学习中，我们使用许多不同形式的规范化。一些最常见的规范化形式旨在修改值，使它们的总和为1。

L1规范化（最小绝对偏差）通过确保每一行的绝对值之和为1来工作。 
L2归一化（指的是最小二乘）通过确保平方和为1来起作用。

    在字面上，我们将“最小二乘”拆分成两个词语，就是“最小”和“二乘”。 其中，“二乘”指的就是以平方的方式来衡量预测值与实际值之间的差异，也就是误差平方。 “最小”就好理解了，就是说预测模型中各参数要使得预测值与实际值之间的误差平方和最小。

通常，L1归一化技术被认为比L2归一化技术更健壮。 L1归一化技术很强大，因为它可以抵抗数据中的异常值。很多时候，数据倾向于包含离群值，而我们对此无能为力。我们希望使用可以在计算过程中安全有效地忽略它们的技术。如果我们要解决离群值很重要的问题，那么L2规范化可能会成为更好的选择。

# Label encoding
